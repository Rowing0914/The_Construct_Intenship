## Tasks

1. Pushing: Move a box to the target position
2. Sliding: Hit a puck and gets it stop around the target position
3. Pick and Place: pick an object and place it onto the target position in the air

## Data

- **States**: The state of the system is represented in the MuJoCo physics engine and consists of angles and velocities of all robot joints as well as positions, rotations and velocities (linear and angular) of all objects.
- **Goals**: Goals describe the **desired position** of the object (a box or a puck depending on the task) with some fixed tolerance.

- **Rewards**: We use binary and sparse rewards by Computing distance between goal and the achieved goal and if it satisfies threshold then return r(s, a, g) = -1 otherwise 0

  ```python
  # https://github.com/openai/gym/blob/master/gym/envs/robotics/fetch_env.py#L53
  def compute_reward(self, achieved_goal, goal, info):
      # Compute distance between goal and the achieved goal.
      d = goal_distance(achieved_goal, goal)
      if self.reward_type == 'sparse':
          return -(d > self.distance_threshold).astype(np.float32)
      else:
          return -d
  ```

- **Observations**: In this paragraph relative means relative to the current gripper position. The policy(Actor) is given as input the absolute position of the gripper, the relative position of the object and the target , as well as the distance between the fingers. The Q-function(Critic) is additionally given the linear velocity of the gripper and fingers as well as relative linear and angular velocity of the object.

  - observations

    ```python
    # https://github.com/openai/gym/blob/master/gym/envs/robotics/fetch_env.py#L112
    obs = np.concatenate([
         grip_pos,                 # the absolute position of the gripper
         object_pos.ravel(),       # the absolute position of the object
         object_rel_pos.ravel(),   # the relative position of the object
         gripper_state,            # gripper state
         object_rot.ravel(),       # the object's rotations
         object_velp.ravel(),      # the object's velocities
         object_velr.ravel(),      # the object's velocities
         grip_velp,                # the distance between the fingers
         gripper_vel,              # the gripper's velocities
        ])
    ```

  - achieved_goal

    ```python
    # https://github.com/openai/gym/blob/master/gym/envs/robotics/fetch_env.py#L108
    if not self.has_object:
        achieved_goal = grip_pos.copy()
    else:
        achieved_goal = np.squeeze(object_pos.copy())
    ```

  - desired_goal

    ```python
    # https://github.com/openai/gym/blob/master/gym/envs/robotics/fetch_env.py#L155
    def _sample_goal(self):
        if self.has_object:
            goal = self.initial_gripper_xpos[:3]+self.np_random.uniform( self.target_range,self.target_range,size=3)
            goal += self.target_offset
            goal[2] = self.height_offset
            if self.target_in_the_air and self.np_random.uniform() < 0.5:
                goal[2] += self.np_random.uniform(0, 0.45)
        else:
            goal = self.initial_gripper_xpos[:3] + self.np_random.uniform(-0.15, 0.15, size=3)
        return goal.copy()
    ```

- **Actions**: None of the problems we consider require gripper rotation and therefore we keep it fixed. Action space is **4-dimensional**. Three dimensions specify the desired relative gripper position at the next time-step.  The last dimension specifies the desired distance between the 2 fingers which are position controlled.

  ```python
  # https://github.com/openai/gym/blob/master/gym/envs/robotics/fetch_env.py#L70
  def _set_action(self, action):
          assert action.shape == (4,)
          action = action.copy()  # ensure that we don't change the action outside of this scope
          pos_ctrl, gripper_ctrl = action[:3], action[3]
  
          pos_ctrl *= 0.05  # limit maximum change in position
          rot_ctrl = [1., 0., 1., 0.]  # fixed rotation of the end effector, expressed as a quaternion
          gripper_ctrl = np.array([gripper_ctrl, gripper_ctrl])
          assert gripper_ctrl.shape == (2,)
          if self.block_gripper:
              gripper_ctrl = np.zeros_like(gripper_ctrl)
          action = np.concatenate([pos_ctrl, rot_ctrl, gripper_ctrl])
  
          # Apply action to simulation.
          utils.ctrl_set_action(self.sim, action)
          utils.mocap_set_action(self.sim, action)
  ```



## Training Procedure

```shell
Initialize Critic and Actor Neural Networks
B := Replay Buffer

For epoch = 1..EPOCHS:
    For episodes = 1..EPISODES:
        g := Sample random goal status
        st_0 := Sample initial status
        s_0 := (st_0, g) # concatenate state and goal

        For t = 0..T:
            a_t = Sample action using epsilon greedy policy according to actor policy
            s_{t+1}, r_t = Observe state and reward after action a_t on state s_t

        For t = 0..T:
            Add to B experience (s_t|g, a_t, r_t, s_{t+1}|g)

            Sample status positions in current episode st_r:
                Add to B this experience with different goal and updated reward

    For t=1..TRAIN:
        MB := Sample a minibatch from B
        Perform an optimization step on critic policy using MB

    If K epoch has passed since last actor update:
        Update actor weights with critic weights
```

