{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hindsight Experience Replay(HER)\n",
    "## Agenda\n",
    "1. Profile of Paper\n",
    "2. Abstract\n",
    "3. Why we developed the algorithm by ourselves\n",
    "4. Hands-on HER on ROSDS\n",
    "    - Components\n",
    "    - How to launch\n",
    "    - How to modify\n",
    "5. Conclusion\n",
    "\n",
    "## Profile of Paper\n",
    "- Authors: M.Andrychowicz et al.(Open AI)\n",
    "- Year: 2017\n",
    "- Official Video: https://sites.google.com/site/hindsightexperiencereplay/\n",
    "\n",
    "## Abstract\n",
    "explain what it does\n",
    "\n",
    "## Why we developed the algortihm by ourselves\n",
    "explain why we didn't use `baselines`, rather did develop the algorithm by ourselves\n",
    "\n",
    "## Hands-on HER on ROSDS\n",
    "Demonstrate how one can launch the HER learning\n",
    "\n",
    "## Conclusion\n",
    "say thank you to the audience and make some remarks.\n",
    "also you can talk about how to modify it to make a tailored algorithm working on arbitrary problems.\n",
    "\n",
    "## Reference\n",
    "```\n",
    "@inproceedings{andrychowicz2017hindsight,\n",
    "  title={Hindsight experience replay},\n",
    "  author={Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, OpenAI Pieter and Zaremba, Wojciech},\n",
    "  booktitle={Advances in Neural Information Processing Systems},\n",
    "  pages={5048--5058},\n",
    "  year={2017}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./assets/HER_algo.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-54edf9590f08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mHER_DDPG\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \t\"\"\"\n\u001b[1;32m      3\u001b[0m         \u001b[0mDDPG\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mHindsight\u001b[0m \u001b[0mExperience\u001b[0m \u001b[0mReplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \t\"\"\"\n",
      "\u001b[0;32m<ipython-input-2-54edf9590f08>\u001b[0m in \u001b[0;36mHER_DDPG\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;34m@\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mautograph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_select_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "class HER_DDPG:\n",
    "\t\"\"\"\n",
    "\tDDPG for Hindsight Experience Replay\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, actor, critic, num_action, params, o_norm, g_norm):\n",
    "\t\tself.params = params\n",
    "\t\tself.num_action = num_action\n",
    "\t\tself.clip_target = 1 / (1 - self.params.gamma)\n",
    "\t\tself.eval_flg = False\n",
    "\t\tself.index_timestep = 0\n",
    "\t\tself.actor = actor(num_action)\n",
    "\t\tself.critic = critic(1)\n",
    "\t\tself.target_actor = deepcopy(self.actor)\n",
    "\t\tself.target_critic = deepcopy(self.critic)\n",
    "\t\tself.actor_optimizer = tf.train.AdamOptimizer(learning_rate=0.001)  # fixed learning_rate\n",
    "\t\tself.critic_optimizer = tf.train.AdamOptimizer(learning_rate=0.001)  # fixed learning_rate\n",
    "\t\tself.o_norm = o_norm\n",
    "\t\tself.g_norm = g_norm\n",
    "\n",
    "\tdef predict(self, obs, g):\n",
    "\t\tobs = self.o_norm.normalise(obs)\n",
    "\t\tg = self.g_norm.normalise(g)\n",
    "\t\tstate = np.concatenate([obs, g], axis=-1)\n",
    "\t\tstate = np.expand_dims(state, axis=0).astype(np.float32)\n",
    "\t\taction = self._select_action(tf.constant(state))\n",
    "\t\treturn action.numpy()[0] * self.params.max_action\n",
    "\n",
    "\t@tf.contrib.eager.defun(autograph=False)\n",
    "\tdef _select_action(self, state):\n",
    "\t\treturn self.actor(state)\n",
    "\n",
    "\tdef update(self, transitions):\n",
    "\t\tobs = self.o_norm.normalise(transitions['obs'])\n",
    "\t\tg = self.g_norm.normalise(transitions['g'])\n",
    "\t\tstates = np.concatenate([obs, g], axis=-1)\n",
    "\t\tnext_obs = self.o_norm.normalise(transitions['obs_next'])\n",
    "\t\tnext_states = np.concatenate([next_obs, g], axis=-1)\n",
    "\t\tactions = transitions['actions']\n",
    "\t\trewards = transitions['r'].flatten()\n",
    "\n",
    "\t\tstates = np.array(states, dtype=np.float32)\n",
    "\t\tnext_states = np.array(next_states, dtype=np.float32)\n",
    "\t\tactions = np.array(actions, dtype=np.float32)\n",
    "\t\trewards = np.array(rewards, dtype=np.float32)\n",
    "\t\treturn self._inner_update(states, actions, rewards, next_states)\n",
    "\n",
    "\t@tf.contrib.eager.defun(autograph=False)\n",
    "\tdef _inner_update(self, states, actions, rewards, next_states):\n",
    "\t\tself.index_timestep = tf.train.get_global_step()\n",
    "\t\t# Update Critic\n",
    "\t\twith tf.GradientTape() as tape:\n",
    "\t\t\t# critic takes as input states, actions so that we combine them before passing them\n",
    "\t\t\tnext_Q = self.target_critic(next_states, self.target_actor(next_states) / self.params.max_action)\n",
    "\t\t\tq_values = self.critic(states, actions / self.params.max_action)\n",
    "\n",
    "\t\t\t# compute the target discounted Q(s', a')\n",
    "\t\t\tY = rewards + self.params.gamma * tf.reshape(next_Q, [-1])\n",
    "\t\t\tY = tf.clip_by_value(Y, -self.clip_target, 0)\n",
    "\t\t\tY = tf.stop_gradient(Y)\n",
    "\n",
    "\t\t\t# Compute critic loss(MSE or huber_loss)\n",
    "\t\t\tcritic_loss = tf.losses.mean_squared_error(Y, tf.reshape(q_values, [-1]))\n",
    "\n",
    "\t\t# get gradients\n",
    "\t\tcritic_grads = tape.gradient(critic_loss, self.critic.trainable_variables)\n",
    "\n",
    "\t\t# apply processed gradients to the network\n",
    "\t\tself.critic_optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_variables))\n",
    "\n",
    "\t\t# Update Actor\n",
    "\t\twith tf.GradientTape() as tape:\n",
    "\t\t\taction = self.actor(states)\n",
    "\t\t\tactor_loss = -tf.math.reduce_mean(self.critic(states, action))\n",
    "\t\t\t# this is where HER's original operation comes in to penalise the excessive magnitude of action\n",
    "\t\t\tactor_loss += self.params.action_l2 * tf.math.reduce_mean(tf.math.square(action / self.params.max_action))\n",
    "\n",
    "\t\t# get gradients\n",
    "\t\tactor_grads = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "\n",
    "\t\t# apply processed gradients to the network\n",
    "\t\tself.actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))\n",
    "\t\treturn np.sum(critic_loss + actor_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_Agent_HER(agent, env, n_trial=1):\n",
    "\t\"\"\"\n",
    "\tEvaluate the trained agent!\n",
    "\t\"\"\"\n",
    "\tsuccesses = list()\n",
    "\tfor ep in range(n_trial):\n",
    "\t\tstate = env.reset()\n",
    "\t\t# obs, achieved_goal, desired_goal in `numpy.ndarray`\n",
    "\t\tobs, ag, dg, rg = state_unpacker(state)\n",
    "\t\tsuccess = list()\n",
    "\t\tfor ts in range(agent.params.num_steps):\n",
    "\t\t\t# env.render()\n",
    "\t\t\taction = agent.predict(obs, dg)\n",
    "\t\t\t# action = action_postprocessing(action, agent.params)\n",
    "\t\t\tnext_state, reward, done, info = env.step(action)\n",
    "\t\t\tsuccess.append(info.get('is_success'))\n",
    "\t\t\t# obs, achieved_goal, desired_goal in `numpy.ndarray`\n",
    "\t\t\tnext_obs, next_ag, next_dg, next_rg = state_unpacker(next_state)\n",
    "\t\t\tobs = next_obs\n",
    "\t\t\tdg = next_dg\n",
    "\t\tsuccesses.append(success)\n",
    "\treturn np.mean(np.array(successes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
